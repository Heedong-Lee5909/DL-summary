# Artificial Neural Networks - Summary

Artificial Neural Networks (ANNs) are inspired by biological neurons and
form the backbone of deep learning.

## Structure of Neural Networks

-   **Artificial Neuron (Perceptron)**:
    -   Takes multiple inputs.\
    -   Computes a weighted sum of inputs.\
    -   Adds a bias term.\
    -   Applies an activation function to produce an output.
-   **Layers in a Neural Network**:
    -   **Input Layer**: Feeds the input data into the network.\
    -   **Hidden Layers**: Intermediate layers that process data.\
    -   **Output Layer**: Produces the final result of the network.

## Forward Propagation

Forward propagation is the process by which data flows through the
network from the input layer to the output layer.

### Mathematical Formulation

For one neuron:

-   Inputs: ( x_1, x_2 )\
-   Weights: ( w_1, w_2 )\
-   Bias: ( b )\
-   Linear combination:

\[ z = w_1x_1 + w_2x_2 + b \]

-   Activation function (e.g., sigmoid):

\[ a = `\sigma`{=tex}(z) = `\frac{1}{1+e^{-z}}`{=tex} \]

### Example

-   Input: ( x_1 = 0.1 )\
-   Weight: ( w_1 = 0.15 )\
-   Bias: ( b_1 = 0.4 )

Step 1: Compute linear combination\
( z = 0.15 `\times 0.1`{=tex} + 0.4 = 0.415 )

Step 2: Apply sigmoid activation\
( a = `\sigma`{=tex}(0.415) `\approx 0.6023`{=tex} )

If there is a second neuron:\
- Input: ( a_1 = 0.6023 )\
- Compute ( z_2 = w_2a_1 + b_2 )\
- Apply sigmoid â†’ Output ( `\approx 0.7153`{=tex} )

## Key Takeaways

-   Neural networks are built from perceptrons.\
-   Layers include input, hidden, and output layers.\
-   Forward propagation computes outputs step by step using weights,
    biases, and activation functions.\
-   Without activation functions, neural networks reduce to linear
    regression.\
-   Activation functions allow learning of complex tasks like image
    classification and language translation.
