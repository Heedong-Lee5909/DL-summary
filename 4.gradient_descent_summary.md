# Gradient Descent - Summary

Gradient descent is an iterative optimization algorithm used to find the
minimum of a function. It is central to training neural networks and
optimizing weights and biases.

## Cost Function

-   To measure how well a weight `w` fits the data, we define a **cost
    function** (loss function).\
-   Example: For data where $z = 2x$, the cost function $J(w)$ is:

$$ J(w) = \sum (z - wx)^2 $$

-   This function is a parabola with a single global minimum.\
-   For this dataset, the best weight is $w = 2$.

## Gradient Descent Algorithm

1.  **Initialization**: Start with a random weight $w_0$.\
2.  **Gradient Computation**: Compute the slope (gradient) of the cost
    function at the current weight.\
3.  **Update Rule**:

$$ w_{t+1} = w_t - \alpha \cdot \nabla J(w_t) $$

-   $\alpha$ = learning rate (controls step size).\
-   $\nabla J(w_t)$ = gradient at current weight.

4.  **Iteration**: Repeat until the cost converges to the minimum (or
    near it).

## Learning Rate Considerations

-   **Large learning rate**: Steps are too big, may overshoot the
    minimum.\
-   **Small learning rate**: Steps are very small, convergence becomes
    slow.\
-   Proper tuning is critical for efficiency and stability.

## Example Walkthrough

-   Start: $w_0 = 0$ → initial line is flat ($z = 0$), cost is high.\
-   After 1st iteration: $w$ moves closer to 2, cost drops sharply.\
-   After 2nd iteration: $w$ gets closer to 2, cost decreases further.\
-   By the 4th iteration: $w \approx 2$, line fits data well.

Each update moves $w$ **proportional to the negative of the gradient**,
always pushing towards the minimum.

## Key Takeaways

-   Gradient descent finds the best weights by minimizing the cost
    function iteratively.\
-   Update rule: $w \leftarrow w - \alpha \cdot \nabla J(w)$.\
-   The choice of learning rate ($\alpha$) is crucial.
    -   Too large → overshoot the minimum.\
    -   Too small → very slow convergence.
